{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d4d1c5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1697853601.py, line 182)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 182\u001b[1;36m\u001b[0m\n\u001b[1;33m    with open(json_file, 'r', encodeing = 'utf8') as slack data:\u001b[0m\n\u001b[1;37m                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import argparse\n",
    "import os\n",
    "import io\n",
    "import shutil\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from pick import pick\n",
    "from time import sleep\n",
    "#import globe\n",
    "import pandas as pd\n",
    "\n",
    "# combine all json file in all-weeks8-9\n",
    "\n",
    "path_channel = \"anonymaized/\"\n",
    "def slack_parser(path_channel):\n",
    "    \"\"\" parse slack data to extract useful informations from the json file\n",
    "        step of execution\n",
    "        1. Import the required modules\n",
    "        2. read all json file from the provided path\n",
    "        3. combine all json files in the provided path\n",
    "        4. extract all required informations from the slack data\n",
    "        5. convert to dataframe and merge all\n",
    "        6. reset the index and return dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # specify path to get json files\n",
    "    combined = []\n",
    "    for json_file in glob.glob(f\"{path_channel}*.json\"):\n",
    "        with open(json_file, 'r', encoding=\"utf8\") as slack_data:\n",
    "            combined.append(slack_data)\n",
    "\n",
    "    # loop through all json files and extract required informations\n",
    "    dflist = []\n",
    "    for slack_data in combined:\n",
    "\n",
    "        msg_type, msg_content, sender_id, time_msg, msg_dist, time_thread_st, reply_users, \\\n",
    "        reply_count, reply_users_count, tm_thread_end = [],[],[],[],[],[],[],[],[],[]\n",
    "\n",
    "        for row in slack_data:\n",
    "            if 'bot_id' in row.keys():\n",
    "                continue\n",
    "            else:\n",
    "                msg_type.append(row['type'])\n",
    "                msg_content.append(row['text'])\n",
    "                if 'user_profile' in row.keys(): sender_id.append(row['user_profile']['real_name'])\n",
    "                else: sender_id.append('Not provided')\n",
    "                time_msg.append(row['ts'])\n",
    "                if 'blocks' in row.keys() and len(row['blocks'][0]['elements'][0]['elements']) != 0 :\n",
    "                     msg_dist.append(row['blocks'][0]['elements'][0]['elements'][0]['type'])\n",
    "                else: msg_dist.append('reshared')\n",
    "                if 'thread_ts' in row.keys():\n",
    "                    time_thread_st.append(row['thread_ts'])\n",
    "                else:\n",
    "                    time_thread_st.append(0)\n",
    "                if 'reply_users' in row.keys(): reply_users.append(\",\".join(row['reply_users'])) \n",
    "                else:    reply_users.append(0)\n",
    "                if 'reply_count' in row.keys():\n",
    "                    reply_count.append(row['reply_count'])\n",
    "                    reply_users_count.append(row['reply_users_count'])\n",
    "                    tm_thread_end.append(row['latest_reply'])\n",
    "                else:\n",
    "                    reply_count.append(0)\n",
    "                    reply_users_count.append(0)\n",
    "                    tm_thread_end.append(0)\n",
    "        data = zip(msg_type, msg_content, sender_id, time_msg, msg_dist, time_thread_st,\n",
    "         reply_count, reply_users_count, reply_users, tm_thread_end)\n",
    "        columns = ['msg_type', 'msg_content', 'sender_name', 'msg_sent_time', 'msg_dist_type',\n",
    "         'time_thread_start', 'reply_count', 'reply_users_count', 'reply_users', 'tm_thread_end']\n",
    "\n",
    "        df = pd.DataFrame(data=data, columns=columns)\n",
    "        df = df[df['sender_name'] != 'Not provided']\n",
    "        dflist.append(df)\n",
    "\n",
    "    dfall = pd.concat(dflist, ignore_index=True)\n",
    "    dfall['channel'] = path_channel.split('/')[-1].split('.')[0]        \n",
    "    dfall = dfall.reset_index(drop=True)\n",
    "    \n",
    "    return dfall\n",
    "\n",
    "def parse_slack_reaction(path, channel):\n",
    "    \"\"\"get reactions\"\"\"\n",
    "    dfall_reaction = pd.DataFrame()\n",
    "    combined = []\n",
    "    for json_file in glob.glob(f\"{path}*.json\"):\n",
    "        with open(json_file, 'r') as slack_data:\n",
    "            combined.append(slack_data)\n",
    "\n",
    "    reaction_name, reaction_count, reaction_users, msg, user_id = [], [], [], [], []\n",
    "\n",
    "    for k in combined:\n",
    "        slack_data = json.load(open(k.name, 'r', encoding=\"utf-8\"))\n",
    "        \n",
    "        for i_count, i in enumerate(slack_data):\n",
    "            if 'reactions' in i.keys():\n",
    "                for j in range(len(i['reactions'])):\n",
    "                    msg.append(i['text'])\n",
    "                    user_id.append(i['user'])\n",
    "                    reaction_name.append(i['reactions'][j]['name'])\n",
    "                    reaction_count.append(i['reactions'][j]['count'])\n",
    "                    reaction_users.append(\",\".join(i['reactions'][j]['users']))\n",
    "                \n",
    "    data_reaction = zip(reaction_name, reaction_count, reaction_users, msg, user_id)\n",
    "    columns_reaction = ['reaction_name', 'reaction_count', 'reaction_users_count', 'message', 'user_id']\n",
    "    df_reaction = pd.DataFrame(data=data_reaction, columns=columns_reaction)\n",
    "    df_reaction['channel'] = channel\n",
    "    return df_reaction\n",
    "\n",
    "def get_community_participation(path):\n",
    "    \"\"\" specify path to get json files\"\"\"\n",
    "    combined = []\n",
    "    comm_dict = {}\n",
    "    for json_file in glob.glob(f\"{path}*.json\"):\n",
    "        with open(json_file, 'r') as slack_data:\n",
    "            combined.append(slack_data)\n",
    "    # print(f\"Total json files is {len(combined)}\")\n",
    "    for i in combined:\n",
    "        a = json.load(open(i.name, 'r', encoding='utf-8'))\n",
    "\n",
    "        for msg in a:\n",
    "            if 'replies' in msg.keys():\n",
    "                for i in msg['replies']:\n",
    "                    comm_dict[i['user']] = comm_dict.get(i['user'], 0)+1\n",
    "    return comm_dict\n",
    "\n",
    "\n",
    "\n",
    "# Create wrapper classes for using slack_sdk in place of slacker\n",
    "class SlackDataLoader: \n",
    "    '''\n",
    "    Slack exported data IO class.\n",
    "\n",
    "    When you open slack exported ZIP file, each channel or direct message \n",
    "    will have its own folder. Each folder will contain messages from the \n",
    "    conversation, organised by date in separate JSON files.\n",
    "\n",
    "    You'll see reference files for different kinds of conversations: \n",
    "    users.json files for all types of users that exist in the slack workspace\n",
    "    channels.json files for public channels, \n",
    "    \n",
    "    These files contain metadata about the conversations, including their names and IDs.\n",
    "\n",
    "    For secruity reason, we have annonymized names - the names you will see are generated using faker library.\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, path):\n",
    "        '''\n",
    "        path: path to the slack exported data folder\n",
    "        '''\n",
    "        self.path = path\n",
    "        self.channels = self.get_channels()\n",
    "        self.users = self.get_ussers()\n",
    "    \n",
    "\n",
    "    def get_users(self):\n",
    "        '''\n",
    "        write a function to get all the users from the json file\n",
    "        '''\n",
    "        with open(os.path.join(self.path, 'users.json'), 'r') as f:\n",
    "            users = json.load(f)\n",
    "\n",
    "        return users\n",
    "    \n",
    "    def get_channels(self):\n",
    "        '''\n",
    "        write a function to get all the channels from the json file\n",
    "        '''\n",
    "        with open(os.path.join(self.path, 'channels.json'), 'r') as f:\n",
    "            channels = json.load(f)\n",
    "\n",
    "        return channels\n",
    "\n",
    "    def get_channel_messages(self, channel_name):\n",
    "        '''\n",
    "        write a function to get all the messages from a channel\n",
    "        \n",
    "        '''\n",
    "        channel_path = os.path.join(self.path, channel_name)\n",
    "        message = []\n",
    "        for json_file in glob.glob(f\"{channel_path}/*.json\"):\n",
    "            with open(json_file, 'r', encodeing = 'utf8') as slack data:\n",
    "                messages += json.load(slack_data)\n",
    "                \n",
    "                return messages\n",
    "\n",
    "    # \n",
    "    def get_user_map(self):\n",
    "        '''\n",
    "        write a function to get a map between user id and user name\n",
    "        '''\n",
    "        userNamesById = {}\n",
    "        userIdsByName = {}\n",
    "        for user in self.users:\n",
    "            userNamesById[user['id']] = user['name']\n",
    "            userIdsByName[user['name']] = user['id']\n",
    "        return userNamesById, userIdsByName      \n",
    "    \n",
    "def top_message_by_replies(df, column, n=10):\n",
    "    top_reply_users = df.groupby('user')[column].sum().nlargest(n)\n",
    "    bottom_reply_users = df.groupby('user')[column].sum().nsmallest(n)\n",
    "\n",
    "    # Top and bottom 10 users by mention count\n",
    "    top_mention_users = df.groupby('user')['mentions'].sum().nlargest(n)\n",
    "    bottom_mention_users = df.groupby('user')['mentions'].sum().nsmallest(n)\n",
    "    \n",
    "    # Top and bottom 10 users by message count\n",
    "    top_message_users = df.groupby('user')['message_count'].sum().nlargest(n)\n",
    "    bottom_message_users = df.groupby('user')['message_count'].sum().nsmallest(n)\n",
    "\n",
    "    # Top and bottom 10 users by reaction count\n",
    "    top_reaction_users = df.groupby('user')['reactions'].sum().nlargest(n)\n",
    "    bottom_reaction_users = df.groupby('user')['reactions'].sum().nsmallest(n)\n",
    "    \n",
    "    # Top 10 messages by replies\n",
    "    top_messages_by_replies = df.nlargest(n, 'replies')\n",
    "\n",
    "    # Top 10 messages by reactions\n",
    "    top_messages_by_reactions = df.nlargest(n, 'reactions')\n",
    "\n",
    "    # Top 10 messages by mentions\n",
    "    top_messages_by_mentions = df.nlargest(n, 'mentions')\n",
    "\n",
    "    # Channel with the highest activity\n",
    "    channel_with_highest_activity = df.groupby('channel').sum()[column].idxmax()\n",
    "\n",
    "    # Channel appearing at the right top corner in the scatter plot\n",
    "    scatter_plot_channel = df.groupby('channel').agg({'message_count': 'sum', 'replies': 'sum', 'reactions': 'sum'}).idxmax().values[0]\n",
    "\n",
    "    # Fraction of messages replied within the first 5 minutes\n",
    "    fraction_replied_within_5mins = len(df[df['reply_time'] <= 5]) / len(df)\n",
    "\n",
    "\n",
    "def top_users_by_count(df, column, n=10, ascending=False):\n",
    "    \"\"\"Get the top users by count for a specific column.\"\"\"\n",
    "    top_users = df.groupby('sender_name')[column].sum().sort_values(ascending=ascending).head(n)\n",
    "    return top_users\n",
    "\n",
    "\n",
    "def top_messages_by_count(df, column, n=10, ascending=False):\n",
    "    \"\"\"Get the top messages by count for a specific column.\"\"\"\n",
    "    top_messages = df.sort_values(by=column, ascending=ascending).head(n)\n",
    "    return top_messages\n",
    "\n",
    "def channel_with_highest_activity(df):\n",
    "    \"\"\"Get the channel with the highest activity.\"\"\"\n",
    "    channel_activity = df['channel'].value_counts()\n",
    "    channel_with_highest_activity = channel_activity.idxmax()\n",
    "    return channel_with_highest_activity\n",
    "\n",
    "def fraction_of_messages_replied_within_time(df, time_threshold):\n",
    "    \"\"\"Calculate the fraction of messages replied within a specified time threshold.\"\"\"\n",
    "    replied_messages = df[~df['reply_users'].isnull()]\n",
    "    replied_within_threshold = replied_messages[replied_messages['msg_sent_time'] - replied_messages['thread_ts'] <= pd.Timedelta(minutes=time_threshold)]\n",
    "    fraction_replied = len(replied_within_threshold) / len(df)\n",
    "    return fraction_replied\n",
    "\n",
    "# Scatter plot of time difference and time of the day\n",
    "    plt.scatter(df['reply_time'], df['time_of_day'], c=df['channel'])\n",
    "    plt.xlabel('Time difference between message and first reply (minutes)')\n",
    "    plt.ylabel('Time of the day (24hr format)')\n",
    "    plt.title('Scatter plot of Time Difference and Time of the Day')\n",
    "    plt.colorbar(label='Channel')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Export Slack history')\n",
    "\n",
    "    \n",
    "    parser.add_argument('--zip', help=\"Name of a zip file to import\")\n",
    "    args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc225f4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 200\u001b[0m\n\u001b[0;32m    196\u001b[0m parser \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mArgumentParser(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExport Slack history\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    199\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--zip\u001b[39m\u001b[38;5;124m'\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName of a zip file to import\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 200\u001b[0m args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\argparse.py:1872\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m argv:\n\u001b[0;32m   1871\u001b[0m     msg \u001b[38;5;241m=\u001b[39m _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munrecognized arguments: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1872\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror(msg \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(argv))\n\u001b[0;32m   1873\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\argparse.py:2630\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m   2628\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_usage(_sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m   2629\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprog\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: message}\n\u001b[1;32m-> 2630\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m2\u001b[39m, _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(prog)s\u001b[39;00m\u001b[38;5;124m: error: \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m%\u001b[39m args)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\argparse.py:2617\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[1;34m(self, status, message)\u001b[0m\n\u001b[0;32m   2615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message:\n\u001b[0;32m   2616\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_message(message, _sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m-> 2617\u001b[0m _sys\u001b[38;5;241m.\u001b[39mexit(status)\n",
      "\u001b[1;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65883e39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
